{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import tifffile\n",
    "from skimage.transform import resize\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Conv2D, Reshape, Input, Conv2DTranspose\n",
    "from keras.layers import Activation, LeakyReLU, BatchNormalization, Dropout, Resizing\n",
    "import dcgan as dcgan\n",
    "import wgan as wgan\n",
    "import prgan as prgan\n",
    "from scores import scores\n",
    "from scores import preprocess_array\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "except:\n",
    "    from keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_DIM = 100  \n",
    "BATCH_SIZE = 20 \n",
    "STEPS_PER_EPOCH = 100\n",
    "EPOCHS = 50\n",
    "STEPS = 5\n",
    "SEED = 40\n",
    "WIDTH, HEIGHT, CHANNELS = 75, 75, 3\n",
    "OPTIMIZER = Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('./data/train.json')\n",
    "train['inc_angle'] = pd.to_numeric(train['inc_angle'],errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = pd.read_json('./data/train.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(train,label=1):\n",
    "    print(train.shape)\n",
    "    train['max'+str(label)] = [np.max(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['maxpos'+str(label)] = [np.argmax(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['min'+str(label)] = [np.min(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['minpos'+str(label)] = [np.argmin(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['med'+str(label)] = [np.median(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['std'+str(label)] = [np.std(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['mean'+str(label)] = [np.mean(np.array(x)) for x in train['band_'+str(label)] ]\n",
    "    train['p25_'+str(label)] = [np.sort(np.array(x))[int(0.25*75*75)] for x in train['band_'+str(label)] ]\n",
    "    train['p75_'+str(label)] = [np.sort(np.array(x))[int(0.75*75*75)] for x in train['band_'+str(label)] ]\n",
    "    train['mid50_'+str(label)] = train['p75_'+str(label)]-train['p25_'+str(label)]\n",
    "\n",
    "    return train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_stats(train,1)\n",
    "train = get_stats(train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_var(name,nbins=50):\n",
    "    minval = train[name].min()\n",
    "    maxval = train[name].max()\n",
    "    plt.hist(train.loc[train.is_iceberg==1,name],range=[minval,maxval],\n",
    "             bins=nbins,color='b',alpha=0.5,label='Boat')\n",
    "    plt.hist(train.loc[train.is_iceberg==0,name],range=[minval,maxval],\n",
    "             bins=nbins,color='r',alpha=0.5,label='Iceberg')\n",
    "    plt.legend()\n",
    "    plt.xlim([minval,maxval])\n",
    "    plt.xlabel(name)\n",
    "    plt.ylabel('Number')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train.loc[train.is_iceberg==1]))\n",
    "for col in ['inc_angle','min1','max1','std1','med1','mean1','mid50_1']:\n",
    "    plot_var(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stats = train.drop(['id','is_iceberg','band_1','band_2'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_stats.corr()\n",
    "fig = plt.figure(1, figsize=(10,10))\n",
    "plt.imshow(corr,cmap='inferno')\n",
    "labels = np.arange(len(train_stats.columns))\n",
    "plt.xticks(labels,train_stats.columns,rotation=90)\n",
    "plt.yticks(labels,train_stats.columns)\n",
    "plt.title('Correlation Matrix of Global Variables')\n",
    "cbar = plt.colorbar(shrink=0.85,pad=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icebergs = train[train.is_iceberg==1].sample(n=300,random_state=123)\n",
    "ships = train[train.is_iceberg==0].sample(n=300,random_state=456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot band_1/band_2 images\n",
    "fig = plt.figure(1,figsize=(15,15))\n",
    "for i in range(9):\n",
    "    ax = fig.add_subplot(3,3,i+1)\n",
    "    arr = np.reshape(np.array(icebergs.iloc[i,1]),(75,75))\n",
    "    ax.imshow(arr,cmap='gist_gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(images, n_cols=None):\n",
    "    '''Visualizes fake images'''\n",
    "\n",
    "    n_cols = n_cols or len(images)\n",
    "    n_rows = (len(images) - 1) // n_cols + 1\n",
    "\n",
    "    if images.shape[-1] == 1:\n",
    "        images = np.squeeze(images, axis=-1)\n",
    "\n",
    "    plt.figure(figsize=(n_cols * 2, n_rows * 2))  # Increase the figure size to accommodate 2-band images\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        if image.shape[-1] == 1:\n",
    "            image = np.squeeze(image, axis=-1)\n",
    "        elif image.shape[-1] == 2:\n",
    "            # Merge the two bands into a single image\n",
    "            image = np.dstack((image[:, :, 0], image[:, :, 1]))\n",
    "            image = np.mean(image, axis=2)  # Convert to grayscale by taking the mean of the two bands\n",
    "        else:\n",
    "            raise ValueError(\"Invalid number of image bands\")\n",
    "\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(image, cmap=\"inferno\")\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the training images\n",
    "X_train = icebergs[['band_1', 'band_2']]\n",
    "X_band_1=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_1\"]])\n",
    "X_band_2=np.array([np.array(band).astype(np.float32).reshape(75, 75) for band in train[\"band_2\"]])\n",
    "\n",
    "# adding a new channel\n",
    "X_train = np.concatenate([X_band_1[:, :, :, np.newaxis], X_band_2[:, :, :, np.newaxis],((X_band_1+X_band_2)/2)[:, :, :, np.newaxis]], axis=-1)\n",
    "print(X_train.shape)\n",
    "\n",
    "band_1 = np.array([np.array(image) for image in X_train[:, :, :, 0]])\n",
    "band_2 = np.array([np.array(image) for image in X_train[:, :, :, 1]])\n",
    "band_3 = np.array([np.array(image) for image in X_train[:, :, :, 2]])\n",
    "\n",
    "band_1 = band_1.flatten().reshape(-1)\n",
    "band_2 = band_2.flatten().reshape(-1)\n",
    "band_3 = band_3.flatten().reshape(-1)\n",
    "\n",
    "mean_1 = np.mean(band_1)\n",
    "std_1 = np.std(band_1)\n",
    "\n",
    "mean_2 = np.mean(band_2)\n",
    "std_2 = np.std(band_2)\n",
    "\n",
    "mean_3 = np.mean(band_3)\n",
    "std_3 = np.std(band_3)\n",
    "\n",
    "normalized_band_1 = (band_1 - band_1.min()) / (band_1.max() - band_1.min())\n",
    "normalized_band_2 = (band_2 - band_2.min()) / (band_2.max() - band_2.min())\n",
    "normalized_band_3 = (band_3 - band_3.min()) / (band_3.max() - band_3.min())\n",
    "#normalized_column_1 = (column_1 - mean_1) / std_1\n",
    "#normalized_column_2 = (column_2 - mean_2) / std_2\n",
    "\n",
    "X_train_normalized = np.column_stack((normalized_band_1, normalized_band_2, normalized_band_3))\n",
    "#print(X_train[0])\n",
    "# Reshape images \n",
    "X_train = X_train_normalized.reshape(-1, WIDTH,HEIGHT,CHANNELS)\n",
    "\n",
    "# Convert X_train to a NumPy array\n",
    "X_train_array = np.array(X_train[:,:,:,:2])\n",
    "\n",
    "print(X_train_array.shape)\n",
    "\n",
    "# create batches of tensors to be fed into the model\n",
    "dataset = X_train\n",
    "print(X_train_array.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at a iceberg\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "def plotmy3d(c, name):\n",
    "\n",
    "    data = [\n",
    "        go.Surface(\n",
    "            z=c,\n",
    "            colorscale=[[0, 'black'], [1, 'white']]\n",
    "        )\n",
    "    ]\n",
    "    layout = go.Layout(\n",
    "        title=name,\n",
    "        autosize=False,\n",
    "        width=700,\n",
    "        height=700,\n",
    "        margin=dict(\n",
    "            l=65,\n",
    "            r=50,\n",
    "            b=65,\n",
    "            t=90\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig)\n",
    "plotmy3d(X_band_1[12,:,:], 'Real data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "\n",
    "def sample_images(noise, subplots, generator, prefix, figsize=(22,8), save=False):\n",
    "    generated_images = generator.predict(noise)\n",
    "    print(generated_images.shape)\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    #print(np.amax(generated_images))\n",
    "    samples = []\n",
    "    for i, image in enumerate(generated_images):\n",
    "        plt.subplot(subplots[0], subplots[1], i+1)\n",
    "        if CHANNELS == 1:\n",
    "            plt.imshow(image.reshape((WIDTH, HEIGHT)), cmap='gray')    \n",
    "                                                                            \n",
    "        else:\n",
    "            print(image.shape)\n",
    "            if image.shape == (64, 64, 2):\n",
    "                print(image)\n",
    "                image = ndimage.zoom(image, (75 / 64, 75 / 64, 1), order=1)\n",
    "\n",
    "            image_band1 = np.reshape(np.array(image[:, :, 1]),(75,75))\n",
    "            plt.imshow(image_band1, cmap='gist_gray')\n",
    "\n",
    "            #channel_1 = image[:, :, 0]\n",
    "            #channel_2 = image[:, :, 1]\n",
    "\n",
    "            #plt.subplot(1, 2, 1)\n",
    "            #plt.imshow(channel_1, cmap='gray')\n",
    "            #plt.title('Channel 1')\n",
    "\n",
    "            #plt.subplot(1, 2, 2)\n",
    "            #plt.imshow(channel_2, cmap='gray')\n",
    "            #plt.title('Channel 2')\n",
    "\n",
    "            #plt.show()\n",
    "            import matplotlib.image\n",
    "            #print(np.amin(image.reshape((WIDTH, HEIGHT, 3))))\n",
    "            from skimage import util\n",
    "            #image = (image - image.min()) / (image.max() - image.min())\n",
    "            #print(np.min(image))   # minimum value of the image data\n",
    "            #print(np.max(image))   # maximum value of the image data\n",
    "            #matplotlib.image.imsave('gen_img'+str(i)+'.png', image)\n",
    "            #image_data = np.transpose(image, (2, 0, 1))\n",
    "            # Transpose the array to have shape (height, width, num_channels)\n",
    "            #image_data = np.transpose(image, (2, 0, 1))\n",
    "            #print(image.shape)\n",
    "            # Concatenate the channels along the third axis to create a multi-channel image\n",
    "            print(image.shape)\n",
    "            joined_channels_image = np.concatenate([np.expand_dims(image[:, :, i], axis=0) for i in range(image.shape[2])], axis=0)\n",
    "            joined_channels_image = np.transpose(joined_channels_image, (1, 2, 0))  # Transpose dimensions\n",
    "\n",
    "            samples.append(joined_channels_image)\n",
    "            combined_image = np.mean(joined_channels_image, axis=2)\n",
    "\n",
    "            plt.imshow(combined_image, cmap='gist_gray')\n",
    "            # Save grayscale image as PNG\n",
    "            plt.imsave(prefix + '_gen_img' + str(i) + '.png', combined_image, cmap='inferno')\n",
    "\n",
    "        if save == True:\n",
    "            img_name = \"gen\" + str(i)\n",
    "            plt.savefig(img_name)\n",
    "        plt.subplots_adjust(wspace=None, hspace=None)\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dcgan = np.random.normal(0,1, size=(BATCH_SIZE, NOISE_DIM))\n",
    "noise_wgan = tf.random.normal([32, 75*75*2])\n",
    "generator_dcgan, discriminator_dcgan, dcgan_model = dcgan.build(OPTIMIZER, NOISE_DIM)\n",
    "generator_dcgan_loss_values = dcgan.train(generator_dcgan, discriminator_dcgan, dcgan_model, noise_dcgan, EPOCHS, STEPS, BATCH_SIZE, NOISE_DIM, X_train_array)\n",
    "generator_wgan, generator_wgan_loss_values = wgan.train(EPOCHS, STEPS, BATCH_SIZE, NOISE_DIM, X_train_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generator_dcgan model as an h5 file\n",
    "generator_dcgan.save('generator_dcgan.h5')\n",
    "\n",
    "# Save the generator_wgan model as an h5 file\n",
    "generator_wgan.save('generator_wgan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "importlib.reload(prgan) \n",
    "import prgan as prgan\n",
    "\n",
    "generator_prgan, generator_prgan_loss_values = prgan.build(X_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_prgan.save('generator_prgan.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    " \n",
    "def outlier_removal(array):\n",
    "    if array is not pd.DataFrame:\n",
    "        array = np.array(array)\n",
    "        df = pd.DataFrame(array)\n",
    "    df.head()\n",
    "    print(df.shape)\n",
    "    # IQR\n",
    "    # Calculate the upper and lower limits\n",
    "    Q1 = df.quantile(0.30)\n",
    "    Q3 = df.quantile(0.70)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5*IQR\n",
    "    upper = Q3 + 1.5*IQR\n",
    "    \n",
    "    # Create arrays of Boolean values indicating the outlier rows\n",
    "    upper_array = np.where(df>=upper)[0]\n",
    "    lower_array = np.where(df<=lower)[0]\n",
    "    \n",
    "    # Replace outlier values with NaN\n",
    "    df.iloc[upper_array] = np.nan\n",
    "    df.iloc[lower_array] = np.nan\n",
    "    \n",
    "    # Replace NaN values with the mean\n",
    "    df = df.fillna(df.mean())\n",
    "    \n",
    "    # Print the new shape of the DataFrame\n",
    "    print(\"New Shape: \", df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_generator_prgan_loss_values = outlier_removal(generator_prgan_loss_values)\n",
    "df_generator_prgan_loss_values.rename(columns={df_generator_prgan_loss_values.columns[0]: 'PRGAN'}, inplace=True)\n",
    "\n",
    "df_generator_wgan_loss_values = outlier_removal(generator_wgan_loss_values)\n",
    "df_generator_wgan_loss_values.rename(columns={df_generator_wgan_loss_values.columns[0]: 'WGAN'}, inplace=True)\n",
    "\n",
    "array_generator_dcgan_loss_values = np.array(generator_dcgan_loss_values)\n",
    "df_generator_dcgan_loss_values = pd.DataFrame(array_generator_dcgan_loss_values, columns=['DCGAN'])\n",
    "\n",
    "\n",
    "df_combined = pd.concat([df_generator_dcgan_loss_values, df_generator_wgan_loss_values, df_generator_prgan_loss_values[:50]], axis=1)\n",
    "# Set Seaborn style\n",
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# Calculate the confidence interval\n",
    "# Calculate and add the confidence intervals for each line\n",
    "for column in df_combined.columns:\n",
    "    model = sm.OLS(df_combined[column], sm.add_constant(df_combined.index))\n",
    "    results = model.fit()\n",
    "    predictions = results.get_prediction(sm.add_constant(df_combined.index))\n",
    "    conf_int = predictions.conf_int(alpha=0.05)\n",
    "    \n",
    "    plt.fill_between(df_combined.index, conf_int[:, 0], conf_int[:, 1], alpha=0.3)\n",
    "\n",
    "sns.lineplot(df_combined, ci='sd', markers=False)\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss evolution during the training')\n",
    "\n",
    "# Adding a legend\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.savefig(\"chart-loss.pdf\", format=\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "output = [i for i in range(1, len(df_generator_prgan_loss_values)+1)]\n",
    "print(output)\n",
    "# Scatter plot\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax.scatter(output, df_generator_prgan_loss_values)\n",
    " \n",
    "# x-axis label\n",
    "ax.set_xlabel('(body mass index of people)')\n",
    " \n",
    "# y-axis label\n",
    "ax.set_ylabel('(bp of the people )')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import randn\n",
    "\n",
    "noise_dcgan = np.random.normal(0, 1, size=(100, NOISE_DIM))\n",
    "noise_wgan = tf.random.normal([10, 75*75*2])\n",
    "\n",
    "samples_dcgan = sample_images(noise_dcgan, (10,10), generator_dcgan, 'dcgan', (24,20), save=True)\n",
    "samples_wgan = sample_images(noise_wgan, (10,10), generator_wgan, 'wgan', (24,20), save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_prgan = randn(100 * 10)\n",
    "# reshape into a batch of inputs for the network\n",
    "noise_prgan = noise_prgan.reshape(10, 100)\n",
    "samples_prgan = sample_images(noise_prgan, (10,10), generator_prgan, 'prgan', (24,20), save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(4, 5, figsize=(25, 25))\n",
    "# Titles for each row\n",
    "titles = [\"Real Images\", \"DCGAN\", \"WGAN\", \"PRGAN\"]\n",
    "\n",
    "# Adjust layout\n",
    "plt.subplots_adjust(left=0.05, right=0.95, top=0.92, bottom=0.08, wspace=0.1, hspace=0.0)\n",
    "\n",
    "# Iterate through rows and columns to plot images\n",
    "for i in range(4):\n",
    "    # Set the title for the entire row\n",
    "    \n",
    "    for j in range(5):\n",
    "        if i == 0:\n",
    "            axes[i, j].axis(\"off\")\n",
    "            fig.text(0.02, 0.83 - i * 0.25, titles[i], va='center', ha='center', rotation='vertical', fontsize=25)\n",
    "            axes[i, j].imshow(X_train_array[j][:, :, 0], cmap='gray')\n",
    "            if j == 0:\n",
    "                axes[i, j].axis(\"on\")\n",
    "        elif i == 1:\n",
    "            axes[i, j].axis(\"off\")\n",
    "            fig.text(0.02, 0.86 - i * 0.25, titles[i], va='center', ha='center', rotation='vertical', fontsize=25)\n",
    "            axes[i, j].imshow(samples_dcgan[(j * 2) % len(samples_dcgan)][:, :, 0], cmap='gray')\n",
    "            if j == 0:\n",
    "                axes[i, j].axis(\"on\")\n",
    "        elif i == 2:\n",
    "            axes[i, j].axis(\"off\")\n",
    "            fig.text(0.02, 0.9 - i * 0.25, titles[i], va='center', ha='center', rotation='vertical', fontsize=25)\n",
    "            axes[i, j].imshow(samples_wgan[(j * 2) % len(samples_wgan)][:, :, 0], cmap='gray')\n",
    "            if j == 0:\n",
    "                axes[i, j].axis(\"on\")\n",
    "        elif i == 3:\n",
    "            axes[i, j].axis(\"off\")\n",
    "            fig.text(0.02, 0.94 - i * 0.25, titles[i], va='center', ha='center', rotation='vertical', fontsize=25)\n",
    "            axes[i, j].imshow(samples_prgan[(j * 2) % len(samples_prgan)][:, :, 0], cmap='gray')\n",
    "            if j == 0:\n",
    "                axes[i, j].axis(\"on\")\n",
    "\n",
    "        \n",
    "        axes[i, j].tick_params(axis='both', which='both', labelsize=23, length=8)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dcgan_array = np.array(samples_dcgan)\n",
    "samples_wgan_array = np.array(samples_wgan)\n",
    "samples_prgan_array = np.array(samples_prgan)\n",
    "X_train_array = np.array(X_train)\n",
    "\n",
    "samples_dcgan_array = preprocess_array(samples_dcgan_array)\n",
    "samples_wgan_array = preprocess_array(samples_wgan_array)\n",
    "samples_prgan_array = preprocess_array(samples_prgan_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scores\n",
    "importlib.reload(scores) \n",
    "from scores import scores, calculate_bhattacharyya, calculate_chi_square\n",
    "\n",
    "\n",
    "print(X_train_array[:30].shape)\n",
    "print(samples_dcgan_array[:30].shape)\n",
    "#fid, fid_error, bc, bc_error, chi_square, chi_square_error, correlation, correlation_error, intersection, intersection_error\n",
    "#    return fid, fid_error, bhattacharyya, bhattacharyya_error, chi_square, chi_square_error, correlation, correlation_error, intersection, intersection_error\n",
    "\n",
    "dcgan_fid, dcgan_fid_error, dcgan_bc, dcgan_bc_error, dcgan_cs, dcgan_cs_error, dcgan_corr, dcgan_corr_error, dcgan_inter, dcgan_inter_error = scores(X_train_array[:10], samples_dcgan_array[:10])\n",
    "wgan_fid, wgan_fid_error, wgan_bc, wgan_bc_error, wgan_cs, wgan_cs_error, wgan_corr, wgan_corr_error, wgan_inter, wgan_inter_error = scores(X_train_array[:10], samples_wgan_array[:10])\n",
    "prgan_fid, prgan_fid_error, prgan_bc, prgan_bc_error, prgan_cs, prgan_cs_error, prgan_corr, prgan_corr_error, prgan_inter, prgan_inter_error = scores(X_train_array[:10], samples_prgan_array[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_fid, baseline_fid_error, baseline_bc, baseline_bc_error, baseline_cs, baseline_cs_error, baseline_corr, baseline_corr_error, baseline_inter, baseline_inter_error = scores(X_train_array[:10], X_train_array[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for FID comparison\n",
    "model_names = ['Baseline', 'DCGAN', 'WGAN', 'PRGAN']\n",
    "fid_means = [baseline_fid/1000, dcgan_fid/1000, wgan_fid/1000, prgan_fid/1000]\n",
    "fid_errors = [baseline_fid_error/1000, dcgan_fid_error/1000, wgan_fid_error/1000, prgan_fid_error/1000]\n",
    "print(fid_means)\n",
    "\n",
    "bc_means = [baseline_bc, dcgan_bc, wgan_bc, prgan_bc]\n",
    "bc_errors = [baseline_bc_error, dcgan_bc_error, wgan_bc_error, prgan_bc_error]\n",
    "print(bc_means)\n",
    "\n",
    "cs_means = [baseline_cs, dcgan_cs, wgan_cs, prgan_cs]\n",
    "cs_errors = [baseline_cs_error, dcgan_cs_error, wgan_cs_error, prgan_cs_error]\n",
    "print(cs_means)\n",
    "\n",
    "corr_means = [baseline_corr, dcgan_corr, wgan_corr, prgan_corr]\n",
    "corr_errors = [baseline_corr_error, dcgan_corr_error, wgan_bc_error, prgan_corr_error]\n",
    "print(corr_means)\n",
    "\n",
    "inter_means = [baseline_inter, dcgan_inter, wgan_inter, prgan_inter]\n",
    "inter_errors = [baseline_inter_error/1000, dcgan_inter_error/1000, wgan_inter_error/1000, prgan_inter_error/1000]\n",
    "print(inter_means)\n",
    "\n",
    "\n",
    "# Create a figure with five subplots\n",
    "fig, ((ax1, ax2), (ax3, ax5)) = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Set larger font size\n",
    "font_size = 16\n",
    "\n",
    "# Plotting the FID comparison using a line chart with error bars\n",
    "sns.lineplot(x=model_names, y=fid_means, marker='o', color='red', ax=ax1)\n",
    "ax1.errorbar(x=model_names, y=fid_means, yerr=fid_errors, fmt='none', color='black', capsize=5)\n",
    "ax1.set_ylabel('Frechet Inception Distance (FID)', fontsize=font_size)\n",
    "ax1.text(0, 1.05, 'Lower FID is better', transform=ax1.transAxes, fontsize=font_size)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=font_size - 2)\n",
    "\n",
    "# Plotting the BC comparison using a line chart with error bars\n",
    "sns.lineplot(x=model_names, y=bc_means, marker='o', color='red', ax=ax2)\n",
    "ax2.errorbar(x=model_names, y=bc_means, yerr=bc_errors, fmt='none', color='black', capsize=5)\n",
    "ax2.set_ylabel('Bhattacharyya Coefficient (BC)', fontsize=font_size)\n",
    "ax2.text(0, 1.05, 'Lower BC is better', transform=ax2.transAxes, fontsize=font_size)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=font_size - 2)\n",
    "\n",
    "# Plotting the Chi-Square comparison using a line chart with error bars\n",
    "sns.lineplot(x=model_names, y=cs_means, marker='o', color='red', ax=ax3)\n",
    "ax3.errorbar(x=model_names, y=cs_means, yerr=cs_errors, fmt='none', color='black', capsize=5)\n",
    "ax3.set_ylabel('Chi-Square Coefficient (Chi-C)', fontsize=font_size)\n",
    "ax3.text(0, 1.05, 'Lower Chi-C is better', transform=ax3.transAxes, fontsize=font_size)\n",
    "ax3.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "# Plotting the Intersection comparison using a line chart with error bars\n",
    "sns.lineplot(x=model_names, y=inter_means, marker='o', color='red', ax=ax5)\n",
    "ax5.errorbar(x=model_names, y=inter_means, yerr=inter_errors, fmt='none', color='black', capsize=5)\n",
    "ax5.set_ylabel('Intersection Coefficient (IC)', fontsize=font_size)\n",
    "ax5.text(0, 1.05, 'Higher IC is better', transform=ax5.transAxes, fontsize=font_size)\n",
    "ax5.tick_params(axis='both', which='major', labelsize=font_size)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results.pdf\", format=\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_df_from_images(images):\n",
    "    num_images = len(images)\n",
    "    \n",
    "    # Initialize empty lists for 'band_1' and 'band_2' values as lists\n",
    "    band_1_values = []\n",
    "    band_2_values = []\n",
    "    \n",
    "    # Generate unique IDs for each image, matching the number of images\n",
    "    image_ids = [f\"image_{i}\" for i in range(1, num_images + 1)]\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        # Extract 'band_1' and 'band_2' for each image\n",
    "        band_1 = image[:, :, 0]\n",
    "        band_2 = image[:, :, 1]\n",
    "        # Flatten the 'band_1' and 'band_2' arrays\n",
    "        band_1_flat = band_1.flatten()\n",
    "        band_2_flat = band_2.flatten()\n",
    "        band_1_values.append(band_1_flat)\n",
    "        band_2_values.append(band_2_flat)\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    data = {\n",
    "        'id': image_ids,\n",
    "        'band_1': band_1_values,\n",
    "        'band_2': band_2_values,\n",
    "        'inc_angle': np.zeros(num_images),  # You can set inc_angle to a specific value or calculate it\n",
    "        'is_iceberg': np.zeros(num_images)  # Assuming all images have the label '0' (iceberg)\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Optionally, you can set the 'id' column as the DataFrame index\n",
    "    df.set_index('id', inplace=False)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMBINE REAL AND SYNTHETIC DATASETS\n",
    "combined_train_dcgan = np.concatenate([X_train_array[:, :, :, :2], samples_dcgan], axis=0)\n",
    "combined_train_wgan = np.concatenate([X_train_array[:, :, :, :2], samples_wgan], axis=0)\n",
    "combined_train_prgan = np.concatenate([X_train_array[:, :, :, :2], samples_prgan], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_gan_df = get_df_from_images(samples_dcgan)\n",
    "samples_gan_df.to_json('samples_gan.json', orient='records')\n",
    "\n",
    "samples_gan_df = get_stats(samples_gan_df, 1)\n",
    "samples_gan_df = get_stats(samples_gan_df, 2)\n",
    "\n",
    "print(len(samples_gan_df))\n",
    "\n",
    "num_classes = train['is_iceberg'].nunique()\n",
    "print(f\"Number of distinct values in 'specific_column': {distinct_values_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_json('./data/test.json')\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL TO PERFORM THE DATA CLEANING AND SPLITTING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Extract image arrays, non-image features, and labels\n",
    "X_image = train[['band_1', 'band_2']].values  # Image data\n",
    "X_other_features = train.drop(['id', 'band_1', 'band_2', 'is_iceberg', 'inc_angle'], axis=1)  # Non-image features\n",
    "y = train['is_iceberg'].values  # Labels\n",
    "\n",
    "X_synthetic_image = samples_gan_df[['band_1', 'band_2']].values\n",
    "X_synthetic_other_features = samples_gan_df.drop(['id', 'band_1', 'band_2', 'is_iceberg', 'inc_angle'], axis=1)  # Non-image features\n",
    "y_synthetic = samples_gan_df['is_iceberg'].values  # Labels\n",
    "\n",
    "num_other_features = len(X_other_features.columns)\n",
    "\n",
    "# Preprocess image data (resize, normalize, etc.)\n",
    "X_image = np.vstack([np.hstack(arr) for arr in X_image])\n",
    "X_image = X_image.reshape(-1, 75, 75, 2)\n",
    "\n",
    "X_synthetic_image = np.vstack([np.hstack(arr) for arr in X_synthetic_image])\n",
    "X_synthetic_image = X_synthetic_image.reshape(-1, 75, 75, 2)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_image_train, X_image_test, X_other_train, X_other_test, y_train, y_test = train_test_split(\n",
    "    X_image, X_other_features.values, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Concatenate the image arrays vertically\n",
    "X_image_train = np.vstack((X_image_train, X_synthetic_image))\n",
    "\n",
    "# Concatenate the other feature arrays vertically\n",
    "X_other_train = np.vstack((X_other_train, X_synthetic_other_features))\n",
    "\n",
    "# Concatenate the labels\n",
    "y_train = np.concatenate((y_train, y_synthetic))\n",
    "# Assuming y_test contains the true labels (0 or 1)\n",
    "class_counts = np.bincount(y_test)\n",
    "\n",
    "# Assuming class 0 represents \"Iceberg\" and class 1 represents \"Ship\"\n",
    "iceberg_count = class_counts[0]\n",
    "ship_count = class_counts[1]\n",
    "\n",
    "print(f'Number of Iceberg samples in y_test: {iceberg_count}')\n",
    "print(f'Number of Ship samples in y_test: {ship_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model for image data\n",
    "image_input = Input(shape=(75, 75, 2))\n",
    "conv1 = Conv2D(32, (3, 3), activation='relu')(image_input)\n",
    "pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "conv2 = Conv2D(64, (3, 3), activation='relu')(pool1)\n",
    "pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "flatten = Flatten()(pool2)\n",
    "\n",
    "# Define the model for other features\n",
    "other_input = Input(shape=(num_other_features,))\n",
    "dense1 = Dense(64, activation='relu')(other_input)\n",
    "\n",
    "# Combine the two models\n",
    "merged = concatenate([flatten, dense1])\n",
    "\n",
    "# Add additional layers for classification\n",
    "output = Dense(num_classes, activation='softmax')(merged)\n",
    "\n",
    "# Create the hybrid model\n",
    "model = Model(inputs=[image_input, other_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_image_train, X_other_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate([X_image_test, X_other_test], y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming y_test contains the true labels (0 or 1)\n",
    "#print(y_test)\n",
    "#class_counts = np.bincount(y_test)\n",
    "\n",
    "# Assuming class 0 represents \"Iceberg\" and class 1 represents \"Ship\"\n",
    "#iceberg_count = class_counts[0]\n",
    "#ship_count = class_counts[1]\n",
    "\n",
    "#print(f'Number of Iceberg samples in y_test: {iceberg_count}')\n",
    "#print(f'Number of Ship samples in y_test: {ship_count}')\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.predict([X_image_test, X_other_test])\n",
    "print(predictions)\n",
    "# Convert predicted probabilities to binary labels (0 or 1)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate and print classification report\n",
    "class_names = [\"Iceberg (0)\", \"Ship (1)\"]\n",
    "report = classification_report(y_test, predicted_labels, target_names=class_names)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
